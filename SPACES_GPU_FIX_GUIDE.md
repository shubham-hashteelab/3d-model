# ğŸ”§ Spaces GPU é—®é¢˜å®Œæ•´ä¿®å¤æŒ‡å—

## ğŸ¯ é—®é¢˜è¯Šæ–­ï¼šä½ è¯´å¾—å®Œå…¨æ­£ç¡®ï¼

### é—®é¢˜æ ¹æºåˆ†æ

```python
# event_handlers.py - ä¸»è¿›ç¨‹ä¸­
class EventHandlers:
    def __init__(self):
        self.model_inference = ModelInference()  # âŒ åœ¨ä¸»è¿›ç¨‹åˆ›å»ºå®ä¾‹

# model_inference.py
class ModelInference:
    def __init__(self):
        self.model = None  # âŒ å®ä¾‹å˜é‡ï¼Œè·¨è¿›ç¨‹å…±äº«çŠ¶æ€æœ‰é—®é¢˜
    
    def initialize_model(self, device):
        if self.model is None:
            self.model = load_model()  # ç¬¬ä¸€æ¬¡ï¼šåœ¨å­è¿›ç¨‹åŠ è½½
        else:
            self.model = self.model.to(device)  # ç¬¬äºŒæ¬¡ï¼šğŸ’¥ ä¸»è¿›ç¨‹CUDAæ“ä½œï¼
```

### ä¸ºä»€ä¹ˆç¬¬äºŒæ¬¡ä¼šå¤±è´¥ï¼Ÿ

1. **ç¬¬ä¸€æ¬¡è°ƒç”¨**ï¼š
   - `@spaces.GPU` åœ¨å­è¿›ç¨‹è¿è¡Œ
   - `self.model is None` â†’ åŠ è½½æ¨¡å‹
   - `self.model` ä¿å­˜åœ¨å®ä¾‹ä¸­
   - è¿”å›æ—¶ `prediction.gaussians` åŒ…å« CUDA å¼ é‡
   - **pickle æ—¶å°è¯•åœ¨ä¸»è¿›ç¨‹é‡å»º CUDA å¼ é‡** â†’ ğŸ’¥

2. **ç¬¬äºŒæ¬¡è°ƒç”¨**ï¼ˆå³ä½¿ç¬¬ä¸€æ¬¡æˆåŠŸäº†ï¼‰ï¼š
   - æ–°çš„å­è¿›ç¨‹æˆ–çŠ¶æ€æ··ä¹±
   - `self.model` çŠ¶æ€ä¸ç¡®å®š
   - å°è¯• `.to(device)` æ“ä½œ â†’ ğŸ’¥

## âœ… è§£å†³æ–¹æ¡ˆï¼šä¸¤ä¸ªå…³é”®ä¿®æ”¹

### ä¿®æ”¹ 1ï¼šä½¿ç”¨å…¨å±€å˜é‡ç¼“å­˜æ¨¡å‹ï¼ˆé¿å…å®ä¾‹çŠ¶æ€ï¼‰

**ä¸ºä»€ä¹ˆç”¨å…¨å±€å˜é‡ï¼Ÿ**
- `@spaces.GPU` æ¯æ¬¡åœ¨ç‹¬ç«‹å­è¿›ç¨‹è¿è¡Œ
- å…¨å±€å˜é‡åœ¨å­è¿›ç¨‹å†…æ˜¯å®‰å…¨çš„
- ä¸ä¼šæ±¡æŸ“ä¸»è¿›ç¨‹

### ä¿®æ”¹ 2ï¼šè¿”å›å‰ç§»åŠ¨æ‰€æœ‰ CUDA å¼ é‡åˆ° CPU

**ä¸ºä»€ä¹ˆéœ€è¦ï¼Ÿ**
- Pickle åºåˆ—åŒ–è¿”å›å€¼æ—¶ä¼šå°è¯•é‡å»º CUDA å¼ é‡
- å¿…é¡»ç¡®ä¿è¿”å›çš„æ•°æ®éƒ½åœ¨ CPU ä¸Š

## ğŸ“ å®Œæ•´ä¿®å¤ä»£ç 

### æ–‡ä»¶ï¼š`depth_anything_3/app/modules/model_inference.py`

```python
"""
Model inference module for Depth Anything 3 Gradio app.

Modified for HF Spaces GPU compatibility.
"""

import gc
import glob
import os
from typing import Any, Dict, Optional, Tuple
import numpy as np
import torch

from depth_anything_3.api import DepthAnything3
from depth_anything_3.utils.export.glb import export_to_glb
from depth_anything_3.utils.export.gs import export_to_gs_video


# ========================================
# ğŸ”‘ å…³é”®ä¿®æ”¹ 1ï¼šä½¿ç”¨å…¨å±€å˜é‡ç¼“å­˜æ¨¡å‹
# ========================================
# Global cache for model (used in GPU subprocess)
# This is SAFE because @spaces.GPU runs in isolated subprocess
# Each subprocess gets its own copy of this global variable
_MODEL_CACHE = None


class ModelInference:
    """
    Handles model inference and data processing for Depth Anything 3.
    
    Modified for HF Spaces GPU compatibility - does NOT store state
    in instance variables to avoid cross-process issues.
    """

    def __init__(self):
        """Initialize the model inference handler.
        
        Note: Do NOT store model in instance variable to avoid
        state sharing issues with @spaces.GPU decorator.
        """
        # No instance variables! All state in global or local variables
        pass

    def initialize_model(self, device: str = "cuda"):
        """
        Initialize the DepthAnything3 model using global cache.
        
        This uses a global variable which is safe because:
        1. @spaces.GPU runs in isolated subprocess
        2. Each subprocess has its own global namespace
        3. No state leaks to main process

        Args:
            device: Device to load the model on
            
        Returns:
            Model instance ready for inference
        """
        global _MODEL_CACHE
        
        if _MODEL_CACHE is None:
            # First time loading in this subprocess
            model_dir = os.environ.get(
                "DA3_MODEL_DIR", "depth-anything/DA3NESTED-GIANT-LARGE"
            )
            print(f"ğŸ”„ Loading model from {model_dir}...")
            _MODEL_CACHE = DepthAnything3.from_pretrained(model_dir)
            _MODEL_CACHE = _MODEL_CACHE.to(device)
            _MODEL_CACHE.eval()
            print("âœ… Model loaded and ready on GPU")
        else:
            # Model already cached in this subprocess
            print("âœ… Using cached model")
            # Ensure it's on the correct device (defensive programming)
            _MODEL_CACHE = _MODEL_CACHE.to(device)
        
        return _MODEL_CACHE

    def run_inference(
        self,
        target_dir: str,
        filter_black_bg: bool = False,
        filter_white_bg: bool = False,
        process_res_method: str = "upper_bound_resize",
        show_camera: bool = True,
        selected_first_frame: Optional[str] = None,
        save_percentage: float = 30.0,
        num_max_points: int = 1_000_000,
        infer_gs: bool = False,
        gs_trj_mode: str = "extend",
        gs_video_quality: str = "high",
    ) -> Tuple[Any, Dict[int, Dict[str, Any]]]:
        """
        Run DepthAnything3 model inference on images.
        
        This method is wrapped with @spaces.GPU in app.py.

        Args:
            target_dir: Directory containing images
            filter_black_bg: Whether to filter black background
            filter_white_bg: Whether to filter white background
            process_res_method: Method for resizing input images
            show_camera: Whether to show camera in 3D view
            selected_first_frame: Selected first frame filename
            save_percentage: Percentage of points to save (0-100)
            num_max_points: Maximum number of points
            infer_gs: Whether to infer 3D Gaussian Splatting
            gs_trj_mode: Trajectory mode for GS
            gs_video_quality: Video quality for GS

        Returns:
            Tuple of (prediction, processed_data)
        """
        print(f"Processing images from {target_dir}")

        # Device check
        device = "cuda" if torch.cuda.is_available() else "cpu"
        device = torch.device(device)
        print(f"Using device: {device}")

        # ğŸ”‘ ä½¿ç”¨è¿”å›å€¼ï¼Œè€Œä¸æ˜¯ self.model
        model = self.initialize_model(device)

        # Get image paths
        print("Loading images...")
        image_folder_path = os.path.join(target_dir, "images")
        all_image_paths = sorted(glob.glob(os.path.join(image_folder_path, "*")))

        # Filter for image files
        image_extensions = [".jpg", ".jpeg", ".png", ".bmp", ".tiff", ".tif"]
        all_image_paths = [
            path
            for path in all_image_paths
            if any(path.lower().endswith(ext) for ext in image_extensions)
        ]

        print(f"Found {len(all_image_paths)} images")

        # Apply first frame selection logic
        if selected_first_frame:
            selected_path = None
            for path in all_image_paths:
                if os.path.basename(path) == selected_first_frame:
                    selected_path = path
                    break

            if selected_path:
                image_paths = [selected_path] + [
                    path for path in all_image_paths if path != selected_path
                ]
                print(f"User selected first frame: {selected_first_frame}")
            else:
                image_paths = all_image_paths
                print(f"Selected frame not found, using default order")
        else:
            image_paths = all_image_paths

        if len(image_paths) == 0:
            raise ValueError("No images found. Check your upload.")

        # Map UI options to actual method names
        method_mapping = {"high_res": "lower_bound_resize", "low_res": "upper_bound_resize"}
        actual_method = method_mapping.get(process_res_method, "upper_bound_crop")

        # Run model inference
        print(f"Running inference with method: {actual_method}")
        with torch.no_grad():
            # ğŸ”‘ ä½¿ç”¨å±€éƒ¨å˜é‡ modelï¼Œä¸æ˜¯ self.model
            prediction = model.inference(
                image_paths, export_dir=None, process_res_method=actual_method, infer_gs=infer_gs
            )

        # Export to GLB
        export_to_glb(
            prediction,
            filter_black_bg=filter_black_bg,
            filter_white_bg=filter_white_bg,
            export_dir=target_dir,
            show_cameras=show_camera,
            conf_thresh_percentile=save_percentage,
            num_max_points=int(num_max_points),
        )

        # Export to GS video if needed
        if infer_gs:
            mode_mapping = {"extend": "extend", "smooth": "interpolate_smooth"}
            print(f"GS mode: {gs_trj_mode}; Backend mode: {mode_mapping[gs_trj_mode]}")
            export_to_gs_video(
                prediction,
                export_dir=target_dir,
                chunk_size=4,
                trj_mode=mode_mapping.get(gs_trj_mode, "extend"),
                enable_tqdm=True,
                vis_depth="hcat",
                video_quality=gs_video_quality,
            )

        # Save predictions cache
        self._save_predictions_cache(target_dir, prediction)

        # Process results
        processed_data = self._process_results(target_dir, prediction, image_paths)

        # ========================================
        # ğŸ”‘ å…³é”®ä¿®æ”¹ 2ï¼šè¿”å›å‰ç§»åŠ¨æ‰€æœ‰ CUDA å¼ é‡åˆ° CPU
        # ========================================
        print("Moving all tensors to CPU for safe return...")
        prediction = self._move_prediction_to_cpu(prediction)

        # Clean up GPU memory
        torch.cuda.empty_cache()

        return prediction, processed_data

    def _move_prediction_to_cpu(self, prediction: Any) -> Any:
        """
        Move all CUDA tensors in prediction to CPU for safe pickling.
        
        This is CRITICAL for HF Spaces with @spaces.GPU decorator.
        Without this, pickle will try to reconstruct CUDA tensors in
        the main process, causing CUDA initialization error.
        
        Args:
            prediction: Prediction object that may contain CUDA tensors
            
        Returns:
            Prediction object with all tensors moved to CPU
        """
        # Move gaussians tensors to CPU
        if hasattr(prediction, 'gaussians') and prediction.gaussians is not None:
            gaussians = prediction.gaussians
            
            # Move each tensor attribute to CPU
            tensor_attrs = ['means', 'scales', 'rotations', 'harmonics', 'opacities']
            for attr in tensor_attrs:
                if hasattr(gaussians, attr):
                    tensor = getattr(gaussians, attr)
                    if isinstance(tensor, torch.Tensor) and tensor.is_cuda:
                        setattr(gaussians, attr, tensor.cpu())
                        print(f"  âœ“ Moved gaussians.{attr} to CPU")
        
        # Move any tensors in aux dict to CPU
        if hasattr(prediction, 'aux') and prediction.aux is not None:
            for key, value in list(prediction.aux.items()):
                if isinstance(value, torch.Tensor) and value.is_cuda:
                    prediction.aux[key] = value.cpu()
                    print(f"  âœ“ Moved aux['{key}'] to CPU")
                elif isinstance(value, dict):
                    # Recursively handle nested dicts
                    for k, v in list(value.items()):
                        if isinstance(v, torch.Tensor) and v.is_cuda:
                            value[k] = v.cpu()
                            print(f"  âœ“ Moved aux['{key}']['{k}'] to CPU")
        
        print("âœ… All tensors moved to CPU")
        return prediction

    def _save_predictions_cache(self, target_dir: str, prediction: Any) -> None:
        """Save predictions data to predictions.npz for caching."""
        try:
            output_file = os.path.join(target_dir, "predictions.npz")
            save_dict = {}

            if prediction.processed_images is not None:
                save_dict["images"] = prediction.processed_images

            if prediction.depth is not None:
                save_dict["depths"] = np.round(prediction.depth, 6)

            if prediction.conf is not None:
                save_dict["conf"] = np.round(prediction.conf, 2)

            if prediction.extrinsics is not None:
                save_dict["extrinsics"] = prediction.extrinsics
            if prediction.intrinsics is not None:
                save_dict["intrinsics"] = prediction.intrinsics

            np.savez_compressed(output_file, **save_dict)
            print(f"Saved predictions cache to: {output_file}")

        except Exception as e:
            print(f"Warning: Failed to save predictions cache: {e}")

    def _process_results(
        self, target_dir: str, prediction: Any, image_paths: list
    ) -> Dict[int, Dict[str, Any]]:
        """Process model results into structured data."""
        processed_data = {}

        depth_vis_dir = os.path.join(target_dir, "depth_vis")

        if os.path.exists(depth_vis_dir):
            depth_files = sorted(glob.glob(os.path.join(depth_vis_dir, "*.jpg")))
            for i, depth_file in enumerate(depth_files):
                processed_image = None
                if prediction.processed_images is not None and i < len(
                    prediction.processed_images
                ):
                    processed_image = prediction.processed_images[i]

                processed_data[i] = {
                    "depth_image": depth_file,
                    "image": processed_image,
                    "original_image_path": image_paths[i] if i < len(image_paths) else None,
                    "depth": prediction.depth[i] if i < len(prediction.depth) else None,
                    "intrinsics": (
                        prediction.intrinsics[i]
                        if prediction.intrinsics is not None and i < len(prediction.intrinsics)
                        else None
                    ),
                    "mask": None,
                }

        return processed_data

    def cleanup(self) -> None:
        """Clean up GPU memory."""
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
        gc.collect()
```

## ğŸ” å…³é”®å˜åŒ–æ€»ç»“

### Before (æœ‰é—®é¢˜)ï¼š
```python
class ModelInference:
    def __init__(self):
        self.model = None  # âŒ å®ä¾‹å˜é‡
    
    def initialize_model(self, device):
        if self.model is None:
            self.model = load_model()  # âŒ ä¿å­˜åœ¨å®ä¾‹ä¸­
        else:
            self.model = self.model.to(device)  # âŒ è·¨è¿›ç¨‹æ“ä½œ

def run_inference(self):
        self.initialize_model(device)  # âŒ ä½¿ç”¨å®ä¾‹æ–¹æ³•
        prediction = self.model.inference(...)  # âŒ ä½¿ç”¨å®ä¾‹å˜é‡
        return prediction  # âŒ åŒ…å« CUDA å¼ é‡
```

### After (æ­£ç¡®)ï¼š
```python
_MODEL_CACHE = None  # âœ… å…¨å±€å˜é‡ï¼ˆå­è¿›ç¨‹å®‰å…¨ï¼‰

class ModelInference:
    def __init__(self):
        pass  # âœ… æ— å®ä¾‹å˜é‡
    
    def initialize_model(self, device):
        global _MODEL_CACHE
        if _MODEL_CACHE is None:
            _MODEL_CACHE = load_model()  # âœ… ä¿å­˜åœ¨å…¨å±€
        return _MODEL_CACHE  # âœ… è¿”å›è€Œä¸æ˜¯å­˜å‚¨

    def run_inference(self):
        model = self.initialize_model(device)  # âœ… å±€éƒ¨å˜é‡
        prediction = model.inference(...)  # âœ… ä½¿ç”¨å±€éƒ¨å˜é‡
        prediction = self._move_prediction_to_cpu(prediction)  # âœ… ç§»åˆ° CPU
        return prediction  # âœ… å®‰å…¨è¿”å›
```

## ğŸ¯ ä¸ºä»€ä¹ˆè¿™æ ·ä¿®æ”¹ï¼Ÿ

### 1. å…¨å±€å˜é‡ vs å®ä¾‹å˜é‡

| æ–¹å¼ | é—®é¢˜ | åŸå›  |
|------|------|------|
| `self.model` | âŒ è·¨è¿›ç¨‹çŠ¶æ€æ··ä¹± | å®ä¾‹åœ¨ä¸»è¿›ç¨‹åˆ›å»º |
| `_MODEL_CACHE` | âœ… å­è¿›ç¨‹å†…å®‰å…¨ | æ¯ä¸ªå­è¿›ç¨‹ç‹¬ç«‹ |

### 2. è¿”å› CPU å¼ é‡

```python
# âŒ ç›´æ¥è¿”å›ä¼šæŠ¥é”™
return prediction  # prediction.gaussians.means is on CUDA

# âœ… ç§»åˆ° CPU åè¿”å›
prediction = move_to_cpu(prediction)
return prediction  # All tensors are on CPU, pickle safe
```

## ğŸ§ª æµ‹è¯•ä¿®å¤

```bash
# 1. åº”ç”¨ä¿®æ”¹
# å¤åˆ¶ä¸Šé¢çš„å®Œæ•´ä»£ç åˆ° model_inference.py

# 2. æ¨é€åˆ° Spaces
git add depth_anything_3/app/modules/model_inference.py
git commit -m "Fix: Spaces GPU CUDA initialization error"
git push

# 3. æµ‹è¯•å¤šæ¬¡è¿è¡Œ
# åœ¨ Space ä¸­è¿ç»­è¿è¡Œ 2-3 æ¬¡æ¨ç†
# åº”è¯¥ä¸å†å‡ºç° CUDA é”™è¯¯
```

## ğŸ“Š ä¿®å¤æ•ˆæœ

| é—®é¢˜ | Before | After |
|------|--------|-------|
| ç¬¬ä¸€æ¬¡æ¨ç† | âŒ CUDA é”™è¯¯ | âœ… æ­£å¸¸ |
| ç¬¬äºŒæ¬¡æ¨ç† | âŒ CUDA é”™è¯¯ | âœ… æ­£å¸¸ |
| è¿ç»­æ¨ç† | âŒ å¤±è´¥ | âœ… ç¨³å®š |
| æ¨¡å‹åŠ è½½ | æ¯æ¬¡é‡æ–°åŠ è½½ | ç¼“å­˜å¤ç”¨ |

## ğŸ’¡ æœ€ä½³å®è·µ

å¯¹äº `@spaces.GPU` è£…é¥°çš„å‡½æ•°ï¼š

1. âœ… ä½¿ç”¨**å…¨å±€å˜é‡**ç¼“å­˜æ¨¡å‹ï¼ˆå­è¿›ç¨‹å®‰å…¨ï¼‰
2. âœ… **ä¸è¦**ä½¿ç”¨å®ä¾‹å˜é‡å­˜å‚¨æ¨¡å‹
3. âœ… è¿”å›å‰**ç§»åŠ¨æ‰€æœ‰å¼ é‡åˆ° CPU**
4. âœ… æ¸…ç† GPU å†…å­˜ (`torch.cuda.empty_cache()`)
5. âŒ **ä¸è¦**åœ¨ä¸»è¿›ç¨‹ä¸­åˆå§‹åŒ– CUDA
6. âŒ **ä¸è¦**è¿”å› CUDA å¼ é‡

## ğŸ”— ç›¸å…³èµ„æº

- [HF Spaces Zero GPU æ–‡æ¡£](https://huggingface.co/docs/hub/spaces-gpus#zero-gpu)
- [PyTorch Multiprocessing](https://pytorch.org/docs/stable/notes/multiprocessing.html)
- [Pickle åè®®](https://docs.python.org/3/library/pickle.html)

